apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: postgres-sink-connector
  namespace: amq-streams-cdc
  labels:
    strimzi.io/cluster: cdc-connect-cluster
    app.kubernetes.io/part-of: amq-streams-cdc-demo
spec:
  class: io.confluent.connect.jdbc.JdbcSinkConnector
  tasksMax: 1
  config:
    # Topics to consume
    topics: dbserver1.inventory.products
    
    # JDBC connection configuration
    connection.url: "jdbc:postgresql://postgres:5432/inventory"
    connection.user: "postgres"
    connection.password: "postgres"
    
    # Insert mode
    insert.mode: "upsert"
    pk.mode: "record_key"
    pk.fields: "id"
    
    # Table settings
    table.name.format: "products"
    auto.create: "false"
    auto.evolve: "false"
    
    # Converters - Debezium uses JSON with schemas
    key.converter: org.apache.kafka.connect.json.JsonConverter
    key.converter.schemas.enable: true
    value.converter: org.apache.kafka.connect.json.JsonConverter
    value.converter.schemas.enable: true
    
    # Transforms to extract the 'after' field from Debezium envelope
    transforms: "unwrap"
    transforms.unwrap.type: "io.debezium.transforms.ExtractNewRecordState"
    transforms.unwrap.drop.tombstones: "true"
    
    # Error handling
    errors.tolerance: all
    errors.log.enable: true
    errors.log.include.messages: true
